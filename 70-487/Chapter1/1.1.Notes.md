# Chapter 1 - Accessing data

## Part 1 - Choose data access technologies

The exam covers ADO.net, Entity Framework and WCF data services as data-access technologies.

### Choosing a technology

The different services can be used to achieve the same thing, but take a long time to set up. WCF is heavyweight - unecessary for a one-user scenario.

**ADO.net** (an overhaul of Active-X data objects) is older tech. The .net overhaul has a *disconnected model* (previously a *connected model, in ADO*). This means connections are opened and closed as soon as a task (INSERT, UPDATE, DELETE, etc) is complete.

This *closed model* helps mitigate DB locking which affects concurrency and also helps mitigate against connection limits, as there are a finite number of connections allowed in a DB.

ADO.net also has connection pooling where instead of opening up 10,000 connections for 10,000 request (or just 1 for all 10,000), they are pooled into groups (eg, of 50) and batched up.

ADO also manages active connections between 0 and a limit (default 100) depending on demand.

ADO.net is compatible **cross-platform** - libraries are included in `System.Data` for OracleClient or SqlClient and generic OleDb/Odbc.

**ADO.net architecture** like all other DBs involves connecting, executing commands and holding data.

**Data providers** are for read-only and forward-only access to data. There are a number of different providers, underlying interfaces and implementations:

* DbConnection is a provider based on the IDbConnection interface. Examples are SqlConnection or OracleConnection. It should be closed as soon as possible.
* DbCommand is a provider based on the IDbCommand interface. As per connections, you have SqlCommand, EntityCommand, etc. It governs the DB interactions and should be used with the Parameters collection for parameterised queries.
* DbDataReader is a provider based on the IDataReader interface, with OdbcDataReader and OleDbDataReader examples. It is a forward-only reader similar to a Stream with read-only access.
* DbDataAdapter, interface IDbDataAdapter, implemented by SqlDataAdapter is used with a Connection and a Command object to fill a DataSet/DataTable. It allows writes and changes can be batched up.
* DataSet is a regular .net object with no interface. It is a collection of DataTable objects with relevant metadata & relationships.
* DataTable is another regular .net object and relates to a view of data from a SELECT query or from .net code. It tracks data for write purposes.

### DataSet vs DataReader

You can use a DataReader or a DataAdapter to query data. Every SELECT query in ADO.net uses a DataReader.

When using a DataAdapter, ie. the fill method of a SqlDataAdapter, a DataReader is used to populate the returned DataSet or DataTable.

The DataAdapter uses a DataReader - the DataReader will always be faster. Example code could be used to illustrate this and reinforce how DataAdapter is just a convenient wrapper around a DataReader.

It is important to close connections to the DB either with a **try/catch/finally**, a **try/finally** or a `Using` statement (where the object implements `IDisposable`). In a try-catch, only the `finally` is guaranteed to be called.

* A DataReader can be faster than a DataAdapter with performant code.
* DataReaders have async methods but DataAdapters only have synchronous methods. Can improve UX on large datasets
* DataAdapter.Fill() can only fill DataSets/DataTables - more work required with custom models
* DataSet allows you to mimic RMDB relations
* DataReader requires more work - to count rows you need to do a manual iteration
* DataReader - forward only, one iteration. DataTable - can re-iterate both ways.
* DataSet can be serialised to/from XML, stored in session etc. (though with a large memory footprint).
* DataSet is like an in-memory copy of the DB

**`SqlDataReader` async methods let you tell the client immediately that something is happening, but apart from `.HasRows()\, no way to count without iterating unlike `DataAdapter` which (on completion) has a rows count**

### Why choose ADO.NET

You might choose ADO.NET to support legacy code and because it's stable. Because it's old, there is support for a wide number of DBs and a lots of worked examples of how to use it in different situations.

You can also use it with Azure SQL by just changing the connection string.

### Why choose EF

EF came from the problems with modelling OOP data in SQL structures (impedance mismatch). ORM wrappers came about such as LINQ-to-SQL which weren't great, but EF superceded these despite early criticisms of EF.

EF allows devs to manipulate data as if it were native objects. EF maps entities (ie, Customer, Widget, Accounts) to the data store.

EF has three parts:

* Conceptual modelling (account, widget or customer per above).
* Underlying data store (ie, SQL, but could change).
* The mapping between the two

Models for all three were in seperate files but now exist in one file (`*.edmx`):

* Conceptual schema definition language **CSDL**
* Store schema definition language **SSDL**
* Mapping specification language **MSL**

Whereas in ADO.NET a change from MSSQL to Oracle SQL would mean a re-write, in EF just the mapping/store schema can be changed.

In EF, you can work DB-first, model-first or code-first with the tooling picking up either way. DB first means making the DB or using an existing one then hooking it up with the .edmx file. Model first means building the *conceptual model* first and code-first means writing objects in code to map via EF.

A new EF project starts with an .edmx file and has four main tools:

* Entity model designer - makes the .edmx file, can be used to manage every aspect of the model
* Entity model data wizard - lets you use an existing data store to make the conceptual model
* Create database wizard - builds a DB based on the conceptual model (inverse of model data wizard)
* Update model wizard - updates the DB

**Single vs First - single throws an exception if there aren't 1 items whereas first just gets the first**

The wizard makes all the relevant files when given a DB to connect to.

There are some OOP mapping concepts to know such as Inheritance and Complex types.

There are different ways to map data - **TPH, table per hierarchy** (slightly denormalised, better performance) and **TPT, table per type** (properly normalised, slightly worse performance).

Complex types can be made to map fields to an object - ie, startDate & endDate -> dateRange.

**ObjectContext/DbContext**: DbContext is the new version of the API (ObjectContext still fully usable).

If you want to work with ObjectContext instead, you can downgrade the designer entities, user an older copy of Visual Studio or shoehorn ObjectContext out of DbContext as in `((IObjectContextAdapter)dbContextObject).ObjectContext;`.

ObjectContext generates with `ContextOptions` and `OnContextCreated`. ContextOptions has five props:

* `LazyLoadingEnabled` - true by default, can be bad for performance to lazy-load entities. Lazy loaded data is chatty, eager loading is chunky (fewer trips).
* `ProxyCreationEnabled` - true default, whether proxy objects are created.
* `UseConsistentNullReferenceBehavior`
* `UseCSharpNullComparisonBehavior` - changes linq queries to include "or null"
* `UseLegacyPreserveChangesBehavior`

The ObjectContext class must inherit from EntityObject and has three attributes:

```
[EdmEntityTypeAttribute(NamespaceName = "MyNameSpace", Name = "Customer")]
[Serializable()]
[DataContractAttribute(IsReference = true)]
public partial class Customer : EntityObject
{
 /* ... */
} 
```

An example of using a property for an entity model attribute would be:

```
[EdmScalarPropertyAttribute(EntityKeyProperty=true, IsNullable=false)]
[DataMemberAttribute()]
public global::System.Int32 CustomerId
{
 get
 {
 return _CustomerId;
 }
 set
 {
 if (_CustomerId != value)
 {
 OnCustomerIdChanging(value);
 ReportPropertyChanging("CustomerId");
 _CustomerId = StructuralObject.SetValidValue(value, "CustomerId");
 ReportPropertyChanged("CustomerId");
 OnCustomerIdChanged();
 }
 }
}
private global::System.Int32 _CustomerId;
partial void OnCustomerIdChanging(global::System.Int32 value);
partial void OnCustomerIdChanged();
```

Before/after setting the value, the events (partial methods, here) are called.

**Why choose EF** - there is good tooling for rapid development, you can focus on objects rather than data stores, clear seperation of concerns, works well with WCF and it makes it easy to change data stores.

### Choosing WCF

WCF (windows communication foundation) data services (originally ADO.NET data services) is for opening up OData, REST & JSON and other SOA (service oriented architecture) apps.

**OData** is an open standard for exposing data from applications. JSON is JSON.

To use WCF, you would make a web app and define an EDM with EF then add a data service to the app and enable access.

**Why use WCF**: it is very open, it's HTTP-based so no code is even needed to query. OData is text based so firewall problems are mitigated. Also with WCF, the consumer needn't know SQL or some other tool - they just query it with a RESTful interface.


## Part 2 - Implement caching

Caching is important for performance but thought must be given to how vital it is to not have stale information.

This exam focuses on `ObjectCache` and `HttpContext.Cache`, though there are also ASP.net caching in Session/ViewState, etc and serialisable object like DataSet can be considered cached.

**Windows Azure has built in caching - Co Located takes part of a web-worker's resources to cache, or Dedicated Caching adds a new role type (Cache Worker Role) for caching large amounts of data**

### ObjectCache

You can instantiate it from `MemoryCache.Default`

Exposed methods include:

* Add
* AddOrGetExisting
* Get
* Set

You can check for items in the cache by using an index `cache[item]`. You can set the cache with a variety of setters/getters. They are stored as key-value pairs. The ObjectCache API must return/take `CacheItem` objects.

The `Add` method returns true when a key doesn't exist but false if it exists as it doesn't overwrite an existing CacheItem.

The **CacheItemPolicy** can contain an `AbsoluteExpiration` (TimeSpan) which is a fixed time to clear, or a `SlidingExpiration` (also TimeSpan) which resets every time it is accessed.

Both `System.Web.Caching` and `System.Runtime.Caching` have a slightly different `CacheItemPriority` enumeration. This handles the order in which the cache is cleared.

System.Runtime.Caching's CacheItemPriority can be Default or Not Removable - it stops it being removed by optimisation/memory cleanup utilities.

#### ChangeMonitor

The ObjectCache has a `ChangeMonitor` class with a few different implementations:

* CacheEntryChangeMonitor - the base class for others to build on
* FileChangeMonitor - monitors a file for changes, which are reflected in the cache
* HostFileChangeMonitor - monitors directories and file paths for various changes
* SqlChangeMonitor

### HttpContext.Cache

The `HttpContext.Cache` can be accessed a variety of ways: **HttpContext.Current**, **Page.Cache** or **Page.Context.Cache**. Caching here is just for web applications.

**Unless the exam references HttpContext.Cache, assume it is talking about the ObjectCache**.

The HttpContext.Cache serialises items in System.String/System.Object key-value pairs. The shorthand is simply `Cache[key]`.

A set call `Cache["myKey"] = "myVal";` would overwrite an existing value.

There are a few overloads for inserting in `HttpContext.Current.Cache.Insert(string keyname, object value)`, some of which include a CacheDependency, TimeSpan for expiry or CacheItemPriority.

There is also an `System.Web.Caching.Cache.Add()` method with similar values to the most overloaded `.Insert()` method, but the Add() provides for a callback after an item is Removed (`RemoveCallback`), whereas the Insert() callback `UpdateCallback` is run before an item is removed (and can be used to update a cached item and prevent it from being removed).

`System.Web.Caching`'s `CacheItemPriority` is different to Web.Caching and allows for Low, BelowNormal, Normal (default), AboveNormal, High, NotRemovable and Default (normal) values.

There is also the `CacheDependency` to monitor changes, for instance the `SqlCacheDependency` which monitors the underlying table for changes. This can be instantiated from a `SqlCommand` or with two strings (database from the web.config and the table). 

There are a number of problems and specifics to using the `SqlCacheDependency` such as SqlServer 2005 (for SqlCommand), or the latter requires the table be referenced from a connection string in the web.config.


## Part 3 - Implement transactions

Transactions are important in high-demand applications. System.Transactions, EntityTransaction and SqlTransaction are important to the exam.

DB transactions conform to ACID - atomic, consistent, isolated & durable. The idea is that they either succeed or rollback if any part fails.

Transactions are also **isolated** from each other. Another concept is **distributed transactions** where they can affect multiple systems.

The main transaction namespaces are `System.Transactions`, `System.Data.SqlClient`, `System.Data.EntityClient` and `System.Data.Entity`.

**IsolationLevel** is used to set the level of DB locking for a transaction. There are identical versions of the enum in `System.Data.IsolationLevel` and `System.Transaction.IsolationLevel`.

* Unspecified - depends on the driver
* Chaos - stronger locks take priority
* ReadUncommitted - shared locks not issued; exclusive locks not honoured
* ReadCommitted - shared locks held during reads (no dirty reads), data can be changed before the end of the transaction
* RepeatableRead - locks data from change during read
* Serializable - a range lock on a whold DataSet to prevent writes
* Snapshot - data is copied so one transaction can read while another writes

You can change isolation level during a EntityCommand/SqlCommand/TransactionScope, etc.

### TransactionScope

This has a `Complete()` method to call otherwise the transaction is rolled back. It is also rolled back on exceptions in a using block.

```
using (TransactionScope mainScope = new TransactionScope())
{
 using (SqlConnection firstConnection = new SqlConnection("First"))
 {
 firstConnection.Open();
 using (SqlCommand firstCommand = new SqlCommand("FirstQueryText", firstConnection))
 {
 Int32 recordsAffected = firstCommand.ExecuteNonQuery();
 }
 using (SqlConnection secondConnection = new SqlConnection("Second"))
 {
 secondConnection.Open();
 using (SqlCommand secondCommand = new SqlCommand("SecondQueryText",secondConnection))
 {
 Int32 secondAffected = secondCommand.ExecuteNonQuery();
 }
 }
 }
 mainScope.Complete();
} 
```

In this example, the second query causes the transaction to become distributed.

### EntityTransaction

You can call an EntityCommand in conjuncation with an EntityTransaction - it has `Connection` and `IsolationLevel` properties and `Commit()`/`Rollback()` methods. You don't need to explicitly call it for a transaction to happen.

You can also use it explicitly with `EntityTransaction.BeginTransaction(System.Data.IsolationLevel)`...`EntityTransaction.Commit()`

### SqlTransaction

This is identical to EntityTransaction.


## Part 4 - Implement Data Storage in Windows Azure

Windows Azure storage maps to existing storage techniques and practices. It is similar to Windows Server storage methods.

With Azure, there are several provisos:

1. Network access is a requirement for access to DBs, message queues, etc.
2. Local storage is the same but Table/Blob storage is new

For storage there is the regular SQL DB with a 150gb limit, local storage with 20gb-2tb which is temporary, or azure storage which can be broken down into blob storage of 200gb-1tb (per page blob, up to 100tb per account), table storage with 100tb and queue storage with 100tb.

### Overview

In the exam, the type of data will relate to the storage type - files for blob storage, unstructured tables for table storage, structured tables for SQL DBs, message delivery via queue storage and temporary storage in local (per-instance) storage.

Blobs are regular binary storage, tables are no-sql alternatives and queues are similar to MSMQ with a FIFO structure and have a WCF binding.

### Blobs

Blobs can be accessed anywhere via HTTP/HTTPS, this storage is good for large files, document storage & backup, streaming data and images to view in the browser.

Blobs are stored in: **Storage account** > **container** > **blobs**

They can be accessed via the API (`Microsoft.Windows.Storage, .Auth & .Blob` or by URL:

`http://<storage account name>.blob.core.windows.net/<container>/<blob>`

You can use `StorageCredentials` with `CloudStorageAccount` and `CreateCloudBlobClient` to access the data or use a connection string via the app.config & `ConfigurationManager`:

`<add name="StorageConnection" connectionString="DefaultEndpointsProtocol=https;AccountName=ACCOUNT_NAME_GOES_HERE;AccountKey=ACCOUNT_KEY_GOES_HERE" />`

There are both **Block blobs** and **page blobs**. You use `CloudBlockBlob / GetBlockBlob()` and `CloudPageBlob / GetPageBlock()` to get them.

You can upload streams via `UploadFromStream` and should do so from an async method to not block the main thread.

To delete you call `Delete` or `(bool) blob.DeleteIfExists()`.

You can make a container public or change a private container with the `SetPermisisons` method and call `GetSharedAccessSignature` for temporary access.

### Tables/Queues

Tables are always private. Both tables and queues are accessed via the same CloudStorageAccount object.

Queues don't automatically delete messages - they become invisible for 1 minute (can be refreshed).

You can search through tables:

* TableOperation - fetch a single record
* TableQuery - fetch many records

```
public class Record : TableEntity
{
 public Record() : this(DateTime.UtcNow.ToShortDateString(), Guid.NewGuid().ToString())
 { }
 public Record(string partitionKey, string rowKey)
 {
 this.PartitionKey = partitionKey;
 this.RowKey = rowKey;
 }
 public string FirstName { get; set; }
 public string LastName { get; set; }
}
[TestCase("file")]
public void UploadTableFromConfig(string tableName)
{
 // ARRANGE
 CloudStorageAccount acct = CloudStorageAccount.Parse(ConfigurationManager.ConnectionStrings["StorageConnection"].ConnectionString);
 CloudTableClient client = acct.CreateCloudTableClient();
 var table = client.GetTableReference(tableName);
 Record entity = new Record("1", "asdf"){FirstName = "Fred", LastName = "Flintstone"};
 // ACT
 table.CreateIfNotExists(); // create table
 TableOperation insert = TableOperation.Insert(entity);
 table.Execute(insert); // insert record
 TableOperation fetch = TableOperation.Retrieve<Record>("1", "asdf");
 TableResult result = table.Execute(fetch); // fetch record
 TableOperation del = TableOperation.Delete(result.Result as Record);
 table.Execute(del); // delete record
 // ASSERT
 Assert.That(((Record)result.Result).FirstName, Is.EqualTo("Fred"));
}
```

### The Azure CDN

Good for high demand applications and files, anything static. It must be enabled on a storage account.

The first call will cache a file then performance is higher. Files may be stale - can be controlled with a TTL (time to live) value.

To access CDN content, use the:

`http://<storage account name>.vo.msecnd.net/<container>/<blob>`

url instead of the `...blob.core.windows.net...` url.

### Windows Azure Caching

Caching in Azure is **Role-based**, it can be assigned to a worker or it can be split up (ie, 25% over four workers).

### Handling exceptions / SQL retries

Because Azure SQL is cloud hosted, network delays may cause timeouts - you can implement retries to attempt the call again.

You can inspect the error to see if it's transient and if so, retry (`RetryPolicy / connection.OpenWithRetry()`).


## Part 5 - Create and implement a WCF data service
