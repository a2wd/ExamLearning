# Chapter 1 - Accessing data

## Part 1 - Choose data access technologies

The exam covers ADO.net, Entity Framework and WCF data services as data-access technologies.

### Choosing a technology

The different services can be used to achieve the same thing, but take a long time to set up. WCF is heavyweight - unecessary for a one-user scenario.

**ADO.net** (an overhaul of Active-X data objects) is older tech. The .net overhaul has a *disconnected model* (previously a *connected model, in ADO*). This means connections are opened and closed as soon as a task (INSERT, UPDATE, DELETE, etc) is complete.

This *closed model* helps mitigate DB locking which affects concurrency and also helps mitigate against connection limits, as there are a finite number of connections allowed in a DB.

ADO.net also has connection pooling where instead of opening up 10,000 connections for 10,000 request (or just 1 for all 10,000), they are pooled into groups (eg, of 50) and batched up.

ADO also manages active connections between 0 and a limit (default 100) depending on demand.

ADO.net is compatible **cross-platform** - libraries are included in `System.Data` for OracleClient or SqlClient and generic OleDb/Odbc.

**ADO.net architecture** like all other DBs involves connecting, executing commands and holding data.

**Data providers** are for read-only and forward-only access to data. There are a number of different providers, underlying interfaces and implementations:

* DbConnection is a provider based on the IDbConnection interface. Examples are SqlConnection or OracleConnection. It should be closed as soon as possible.
* DbCommand is a provider based on the IDbCommand interface. As per connections, you have SqlCommand, EntityCommand, etc. It governs the DB interactions and should be used with the Parameters collection for parameterised queries.
* DbDataReader is a provider based on the IDataReader interface, with OdbcDataReader and OleDbDataReader examples. It is a forward-only reader similar to a Stream with read-only access.
* DbDataAdapter, interface IDbDataAdapter, implemented by SqlDataAdapter is used with a Connection and a Command object to fill a DataSet/DataTable. It allows writes and changes can be batched up.
* DataSet is a regular .net object with no interface. It is a collection of DataTable objects with relevant metadata & relationships.
* DataTable is another regular .net object and relates to a view of data from a SELECT query or from .net code. It tracks data for write purposes.

### DataSet vs DataReader

You can use a DataReader or a DataAdapter to query data. Every SELECT query in ADO.net uses a DataReader.

When using a DataAdapter, ie. the fill method of a SqlDataAdapter, a DataReader is used to populate the returned DataSet or DataTable.

The DataAdapter uses a DataReader - the DataReader will always be faster. Example code could be used to illustrate this and reinforce how DataAdapter is just a convenient wrapper around a DataReader.

It is important to close connections to the DB either with a **try/catch/finally**, a **try/finally** or a `Using` statement (where the object implements `IDisposable`). In a try-catch, only the `finally` is guaranteed to be called.

* A DataReader can be faster than a DataAdapter with performant code.
* DataReaders have async methods but DataAdapters only have synchronous methods. Can improve UX on large datasets
* DataAdapter.Fill() can only fill DataSets/DataTables - more work required with custom models
* DataSet allows you to mimic RMDB relations
* DataReader requires more work - to count rows you need to do a manual iteration
* DataReader - forward only, one iteration. DataTable - can re-iterate both ways.
* DataSet can be serialised to/from XML, stored in session etc. (though with a large memory footprint).
* DataSet is like an in-memory copy of the DB

**`SqlDataReader` async methods let you tell the client immediately that something is happening, but apart from `.HasRows()\, no way to count without iterating unlike `DataAdapter` which (on completion) has a rows count**

### Why choose ADO.NET

You might choose ADO.NET to support legacy code and because it's stable. Because it's old, there is support for a wide number of DBs and a lots of worked examples of how to use it in different situations.

You can also use it with Azure SQL by just changing the connection string.

### Why choose EF

EF came from the problems with modelling OOP data in SQL structures (impedance mismatch). ORM wrappers came about such as LINQ-to-SQL which weren't great, but EF superceded these despite early criticisms of EF.

EF allows devs to manipulate data as if it were native objects. EF maps entities (ie, Customer, Widget, Accounts) to the data store.

EF has three parts:

* Conceptual modelling (account, widget or customer per above).
* Underlying data store (ie, SQL, but could change).
* The mapping between the two

Models for all three were in seperate files but now exist in one file (`*.edmx`):

* Conceptual schema definition language **CSDL**
* Store schema definition language **SSDL**
* Mapping specification language **MSL**

Whereas in ADO.NET a change from MSSQL to Oracle SQL would mean a re-write, in EF just the mapping/store schema can be changed.

In EF, you can work DB-first, model-first or code-first with the tooling picking up either way. DB first means making the DB or using an existing one then hooking it up with the .edmx file. Model first means building the *conceptual model* first and code-first means writing objects in code to map via EF.

A new EF project starts with an .edmx file and has four main tools:

* Entity model designer - makes the .edmx file, can be used to manage every aspect of the model
* Entity model data wizard - lets you use an existing data store to make the conceptual model
* Create database wizard - builds a DB based on the conceptual model (inverse of model data wizard)
* Update model wizard - updates the DB

**Single vs First - single throws an exception if there aren't 1 items whereas first just gets the first**

The wizard makes all the relevant files when given a DB to connect to.

There are some OOP mapping concepts to know such as Inheritance and Complex types.

There are different ways to map data - **TPH, table per hierarchy** (slightly denormalised, better performance) and **TPT, table per type** (properly normalised, slightly worse performance).

Complex types can be made to map fields to an object - ie, startDate & endDate -> dateRange.

**ObjectContext/DbContext**: DbContext is the new version of the API (ObjectContext still fully usable).

If you want to work with ObjectContext instead, you can downgrade the designer entities, user an older copy of Visual Studio or shoehorn ObjectContext out of DbContext as in `((IObjectContextAdapter)dbContextObject).ObjectContext;`.

ObjectContext generates with `ContextOptions` and `OnContextCreated`. ContextOptions has five props:

* `LazyLoadingEnabled` - true by default, can be bad for performance to lazy-load entities. Lazy loaded data is chatty, eager loading is chunky (fewer trips).
* `ProxyCreationEnabled` - true default, whether proxy objects are created.
* `UseConsistentNullReferenceBehavior`
* `UseCSharpNullComparisonBehavior` - changes linq queries to include "or null"
* `UseLegacyPreserveChangesBehavior`

The ObjectContext class must inherit from EntityObject and has three attributes:

```
[EdmEntityTypeAttribute(NamespaceName = "MyNameSpace", Name = "Customer")]
[Serializable()]
[DataContractAttribute(IsReference = true)]
public partial class Customer : EntityObject
{
 /* ... */
} 
```

An example of using a property for an entity model attribute would be:

```
[EdmScalarPropertyAttribute(EntityKeyProperty=true, IsNullable=false)]
[DataMemberAttribute()]
public global::System.Int32 CustomerId
{
 get
 {
 return _CustomerId;
 }
 set
 {
 if (_CustomerId != value)
 {
 OnCustomerIdChanging(value);
 ReportPropertyChanging("CustomerId");
 _CustomerId = StructuralObject.SetValidValue(value, "CustomerId");
 ReportPropertyChanged("CustomerId");
 OnCustomerIdChanged();
 }
 }
}
private global::System.Int32 _CustomerId;
partial void OnCustomerIdChanging(global::System.Int32 value);
partial void OnCustomerIdChanged();
```

Before/after setting the value, the events (partial methods, here) are called.

**Why choose EF** - there is good tooling for rapid development, you can focus on objects rather than data stores, clear seperation of concerns, works well with WCF and it makes it easy to change data stores.

### Choosing WCF

WCF (windows communication foundation) data services (originally ADO.NET data services) is for opening up OData, REST & JSON and other SOA (service oriented architecture) apps.

**OData** is an open standard for exposing data from applications. JSON is JSON.

To use WCF, you would make a web app and define an EDM with EF then add a data service to the app and enable access.

**Why use WCF**: it is very open, it's HTTP-based so no code is even needed to query. OData is text based so firewall problems are mitigated. Also with WCF, the consumer needn't know SQL or some other tool - they just query it with a RESTful interface.


## Part 2 - Implement caching

Caching is important for performance but thought must be given to how vital it is to not have stale information.

This exam focuses on `ObjectCache` and `HttpContext.Cache`, though there are also ASP.net caching in Session/ViewState, etc and serialisable object like DataSet can be considered cached.

**Windows Azure has built in caching - Co Located takes part of a web-worker's resources to cache, or Dedicated Caching adds a new role type (Cache Worker Role) for caching large amounts of data**

### ObjectCache

You can instantiate it from `MemoryCache.Default`

Exposed methods include:

* Add
* AddOrGetExisting
* Get
* Set

You can check for items in the cache by using an index `cache[item]`. You can set the cache with a variety of setters/getters. They are stored as key-value pairs. The ObjectCache API must return/take `CacheItem` objects.

The `Add` method returns true when a key doesn't exist but false if it exists as it doesn't overwrite an existing CacheItem.

The **CacheItemPolicy** can contain an `AbsoluteExpiration` (TimeSpan) which is a fixed time to clear, or a `SlidingExpiration` (also TimeSpan) which resets every time it is accessed.

Both `System.Web.Caching` and `System.Runtime.Caching` have a slightly different `CacheItemPriority` enumeration. This handles the order in which the cache is cleared.

System.Runtime.Caching's CacheItemPriority can be Default or Not Removable - it stops it being removed by optimisation/memory cleanup utilities.

#### ChangeMonitor

The ObjectCache has a `ChangeMonitor` class with a few different implementations:

* CacheEntryChangeMonitor - the base class for others to build on
* FileChangeMonitor - monitors a file for changes, which are reflected in the cache
* HostFileChangeMonitor - monitors directories and file paths for various changes
* SqlChangeMonitor

### HttpContext.Cache

The `HttpContext.Cache` can be accessed a variety of ways: **HttpContext.Current**, **Page.Cache** or **Page.Context.Cache**. Caching here is just for web applications.

**Unless the exam references HttpContext.Cache, assume it is talking about the ObjectCache**.

The HttpContext.Cache serialises items in System.String/System.Object key-value pairs. The shorthand is simply `Cache[key]`.

A set call `Cache["myKey"] = "myVal";` would overwrite an existing value.

There are a few overloads for inserting in `HttpContext.Current.Cache.Insert(string keyname, object value)`, some of which include a CacheDependency, TimeSpan for expiry or CacheItemPriority.

There is also an `System.Web.Caching.Cache.Add()` method with similar values to the most overloaded `.Insert()` method, but the Add() provides for a callback after an item is Removed (`RemoveCallback`), whereas the Insert() callback `UpdateCallback` is run before an item is removed (and can be used to update a cached item and prevent it from being removed).

`System.Web.Caching`'s `CacheItemPriority` is different to Web.Caching and allows for Low, BelowNormal, Normal (default), AboveNormal, High, NotRemovable and Default (normal) values.

There is also the `CacheDependency` to monitor changes, for instance the `SqlCacheDependency` which monitors the underlying table for changes. This can be instantiated from a `SqlCommand` or with two strings (database from the web.config and the table). 

There are a number of problems and specifics to using the `SqlCacheDependency` such as SqlServer 2005 (for SqlCommand), or the latter requires the table be referenced from a connection string in the web.config.


## Part 3 - Implement transactions